include:
  - remote: 'https://gitlab.com/cscs-ci/recipes/-/raw/master/templates/v2/.ci-ext.yml'

stages:
  - build
  - test

####################
# global variables #
####################
variables:
  BUILD_DIR: "$CI_PROJECT_DIR/build"       # Custom build directory
  INSTALL_DIR: "$CI_PROJECT_DIR/install"   # Custom install directory
  CMAKE_BUILD_TYPE: "Debug"                # Debug build type
  ARB_GPU: "none"                          # Default to not using GPU
  CUDA_ARCH: "90"                          # Default cuda architecture

###########################
# vcluster specifications #
###########################
.eiger:
  extends: .uenv-runner-eiger-mc
  variables:
    SLURM_CONSTRAINT: mc

.todi:
  extends: .uenv-runner-todi-gh200
  variables:
    SLURM_ACCOUNT: g154
    SLURM_RESERVATION: daint

######################
# uenv specification #
######################
.uenv_image:
  image: arbor/v0.9:latest
  variables:
    UENV_VIEW: 'develop'

#################
# build scripts #
#################
.build_template:
  extends: .uenv_image
  stage: build
  script:
    - mkdir -p $BUILD_DIR
    - mkdir -p $INSTALL_DIR
    - |
      CXX=`which gcc` CC=`which gcc` cmake \
        -S $CI_PROJECT_DIR \
        -B $BUILD_DIR \
        -GNinja \
        -DCMAKE_BUILD_TYPE=$CMAKE_BUILD_TYPE \
        -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR \
        -DBUILD_TESTING=ON \
        -DARB_WITH_ASSERTIONS=ON \
        -DARB_WITH_PROFILING=ON \
        -DARB_VECTORIZE=ON \
        -DARB_WITH_PYTHON=ON \
        -DARB_USE_HWLOC=ON \
        -DARB_WITH_MPI=ON \
        -DARB_GPU=${ARB_GPU} \
        -DCMAKE_CUDA_ARCHITECTURES=${CUDA_ARCH} \
        -DARB_USE_GPU_RNG=ON
    - cmake --build $BUILD_DIR -- -j32
    - cmake --install $BUILD_DIR
  artifacts:
    paths:
      - $BUILD_DIR
      - $INSTALL_DIR

#build_x86_64:
#  extends: [.eiger, .build_template]
#  variables:
#    ARB_GPU: "none"

build_aarch64:
  extends: [.todi, .build_template]
  variables:
    ARB_GPU: "cuda"

################
# test scripts #
################
.test_unit_template:
  extends: .uenv_image
  stage: test
  script:
    - ${BUILD_DIR}/bin/unit-modcc
    - ${BUILD_DIR}/bin/unit-local
    - ${BUILD_DIR}/bin/unit
    - scripts/run_cpp_examples.sh
    - python -m venv --system-site-packages ${INSTALL_DIR}
    - source ${INSTALL_DIR}/bin/activate
    - python -m unittest discover -v -s python
    - scripts/run_python_examples.sh
    - scripts/test_executables.sh
    - deactivate
  variables:
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 1
    SLURM_NTASKS_PER_NODE: 1
    SLURM_TIMELIMIT: "00:30:00"
    SLURM_CPU_BIND: "verbose,none"
    USE_MPI: "NO"

.test_distributed_template:
  extends: .uenv_image
  stage: test
  script:
    - ${BUILD_DIR}/bin/unit-mpi
    - scripts/run_cpp_examples.sh -d
  variables:
    SLURM_JOB_NUM_NODES: 2
    SLURM_CPU_BIND: "verbose,rank_ldom"
    SLURM_TIMELIMIT: "00:30:00"
    USE_MPI: "YES"

#####################
# single node tests #
#####################
#test_x86_64:
#  extends: [.eiger, .test_unit_template]

test_aarch64:
  extends: [.todi, .test_unit_template]

####################
# multi node tests #
####################
#test_x86_64-distributed:
#  extends: [.eiger, .test_distributed_template]
#  variables:
#    SLURM_NTASKS_PER_NODE: 8

test_aarch64-distributed:
  extends: [.todi, .test_distributed_template]
  variables:
    SLURM_GPUS_PER_NODE: 4
    SLURM_GPUS_PER_TASK: 1
    SLURM_NTASKS_PER_NODE: 4
